{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "PCA steps for data downscaling projet, dominant PCs to be exported in an array since that is all we really work with in the main downscalling file (in an effort to reduce memory load)<br>\n",
    "\n",
    "World data preprocessing in this file because i didnt feel as if a new netCDF file was necessary for that data as it already loads rather quickly and doesn't take up much space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import time\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in HRDPS and world data (for now still just working on pressure)\n",
    "P_world= xr.open_mfdataset([\"psl_NAM-22_CCCma-CanESM2_rcp45_r1i1p1_CCCma-CanRCM4_r2_day_20110101-20151231.nc\", \"psl_NAM-22_CCCma-CanESM2_rcp45_r1i1p1_CCCma-CanRCM4_r2_day_20160101-20201231.nc\"])\n",
    "U_world= xr.open_mfdataset([\"uas_NAM-22_CCCma-CanESM2_rcp45_r1i1p1_CCCma-CanRCM4_r2_day_20110101-20151231.nc\", \"uas_NAM-22_CCCma-CanESM2_rcp45_r1i1p1_CCCma-CanRCM4_r2_day_20160101-20201231.nc\"])\n",
    "V_world= xr.open_mfdataset([\"vas_NAM-22_CCCma-CanESM2_rcp45_r1i1p1_CCCma-CanRCM4_r2_day_20110101-20151231.nc\", \"vas_NAM-22_CCCma-CanESM2_rcp45_r1i1p1_CCCma-CanRCM4_r2_day_20160101-20201231.nc\"])\n",
    "hrdps = xr.open_dataset(\"hrdps_day_avgs_postSep2014.nc\")\n",
    "\n",
    "#lets go ahead and trim the hrdps dataset so it fits more snuggly in a year\n",
    "#we're also going to remove the leap days here so that seasonal cycle calcs will work\n",
    "a= hrdps.sel(time_counter=slice('2015-01-01T00:00:00.000000000', '2016-02-28T00:00:00.000000000'))\n",
    "b= hrdps.sel(time_counter=slice('2016-03-01T00:00:00.000000000', '2020-02-28T00:00:00.000000000'))\n",
    "c= hrdps.sel(time_counter=slice('2020-03-01T00:00:00.000000000', '2020-12-31T00:00:00.000000000'))\n",
    "hrdps= xr.concat([a,b,c],dim='time_counter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first lets do some quick processing of the hrdps data\n",
    "hrdps_lat = hrdps.nav_lat.sel(time_counter=hrdps.time_counter[0]).values.flatten()\n",
    "hrdps_lon = hrdps.nav_lon.sel(time_counter=hrdps.time_counter[0]).values.flatten()\n",
    "p_temp = hrdps.atmpres.values\n",
    "u_temp = hrdps.u_wind.values\n",
    "v_temp = hrdps.u_wind.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the hrdps lat and lon\n",
    "np.savetxt(\"hrdps_lat.csv\", hrdps_lat, delimiter=\",\")\n",
    "np.savetxt(\"hrdps_lon.csv\", hrdps_lon, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cummulative time -pressure done (s):1610140592.8061147\n",
      "cummulative time -U wind done (s):1610140596.1753159\n",
      "cummulative time -V wind done (s):1610140600.163743\n"
     ]
    }
   ],
   "source": [
    "#get pressure into a shape we can work in\n",
    "P_hrdps = np.empty((np.shape(p_temp)[1]*np.shape(p_temp)[2],np.shape(p_temp)[0]))\n",
    "for i in range(np.shape(p_temp)[0]):\n",
    "    P_hrdps[:,i] = np.reshape(p_temp[i],(np.shape(p_temp)[1]*np.shape(p_temp)[2],))\n",
    "print(f\"cummulative time -pressure done (s):\"+str(time.time()))\n",
    "    \n",
    "U_hrdps = np.empty((np.shape(u_temp)[1]*np.shape(u_temp)[2],np.shape(u_temp)[0]))\n",
    "for i in range(np.shape(u_temp)[0]):\n",
    "    U_hrdps[:,i] = np.reshape(u_temp[i],(np.shape(u_temp)[1]*np.shape(u_temp)[2],))\n",
    "print(f\"cummulative time -U wind done (s):\"+str(time.time()))\n",
    "    \n",
    "V_hrdps = np.empty((np.shape(v_temp)[1]*np.shape(v_temp)[2],np.shape(v_temp)[0]))\n",
    "for i in range(np.shape(v_temp)[0]):\n",
    "    V_hrdps[:,i] = np.reshape(v_temp[i],(np.shape(v_temp)[1]*np.shape(v_temp)[2],))\n",
    "print(f\"cummulative time -V wind done (s):\"+str(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace all nan with 101325 or 0\n",
    "P_hrdps = (pd.DataFrame(P_hrdps).fillna(101325)).to_numpy()\n",
    "U_hrdps = (pd.DataFrame(U_hrdps).fillna(0)).to_numpy()\n",
    "V_hrdps = (pd.DataFrame(V_hrdps).fillna(0)).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now de-mean the dataset (If the mean isn't removed then it will show up in the mode 1 EOF and give disproportionate weight to the mode 1 percent variance)\n",
    "#calculate seasonal signal for each grid point\n",
    "\n",
    "nyears = 6\n",
    "\n",
    "#pressure\n",
    "seasonal = np.empty((np.shape(P_hrdps)[0],365))\n",
    "for ii in range(len(P_hrdps)):\n",
    "    seasonal[ii,:] = np.mean(np.reshape(P_hrdps[ii,:],(nyears,365)),axis=0)\n",
    "\n",
    "#repeat the seasonal cycle for all years\n",
    "seasonal_all = np.tile(seasonal,(1,nyears))\n",
    "\n",
    "#remove seasonal cycle from the original data (calculate anomalies)\n",
    "P_hrdps_anom = P_hrdps - seasonal_all\n",
    "\n",
    "#U-wind\n",
    "seasonal = np.empty((np.shape(U_hrdps)[0],365))\n",
    "for ii in range(len(U_hrdps)):\n",
    "    seasonal[ii,:] = np.mean(np.reshape(U_hrdps[ii,:],(nyears,365)),axis=0)\n",
    "\n",
    "#repeat the seasonal cycle for all years\n",
    "seasonal_all = np.tile(seasonal,(1,nyears))\n",
    "\n",
    "#remove seasonal cycle from the original data (calculate anomalies)\n",
    "U_hrdps_anom = U_hrdps - seasonal_all\n",
    "\n",
    "#V-wind\n",
    "seasonal = np.empty((np.shape(V_hrdps)[0],365))\n",
    "for ii in range(len(V_hrdps)):\n",
    "    seasonal[ii,:] = np.mean(np.reshape(V_hrdps[ii,:],(nyears,365)),axis=0)\n",
    "\n",
    "#repeat the seasonal cycle for all years\n",
    "seasonal_all = np.tile(seasonal,(1,nyears))\n",
    "\n",
    "#remove seasonal cycle from the original data (calculate anomalies)\n",
    "V_hrdps_anom = V_hrdps - seasonal_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for preprocessing of world data\n",
    "#trim the world dataset to be for right time extent\n",
    "#want days between September 12 2014 and December 31 2020\n",
    "P_world = P_world.sel(time=slice('2014-01-01 12:00:00', '2020-12-31 12:00:00'))\n",
    "U_world = U_world.sel(time=slice('2014-01-01 12:00:00', '2020-12-31 12:00:00'))\n",
    "V_world = V_world.sel(time=slice('2014-01-01 12:00:00', '2020-12-31 12:00:00'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now extract the data you want from the CanRCM4 xarray and trim them according to lat and lon of the HRDPS data\n",
    "#first decide on range you want to work within for CanRCM4 data, want to overlap the HRDPS data by 10% of the max distance (y)\n",
    "maxlon = max(hrdps_lon)\n",
    "minlon = min(hrdps_lon)\n",
    "maxlat = max(hrdps_lat)\n",
    "minlat = min(hrdps_lat)\n",
    "\n",
    "buffer = (maxlat-minlat)*0.15\n",
    "\n",
    "lon = P_world.lon.values.flatten()\n",
    "lat = P_world.lat.values.flatten()\n",
    "\n",
    "index = []\n",
    "\n",
    "#first find idexes that fit into lon range and lat range\n",
    "for i in range(len(lon)):\n",
    "    if lon[i] > (minlon-buffer) and lon[i] < (maxlon+buffer) and lat[i] > (minlat-buffer) and lat[i] < (maxlat+buffer):\n",
    "        index.append(i)\n",
    "\n",
    "#now make new lat, and lon\n",
    "lat_RCM = []\n",
    "lon_RCM = []\n",
    "\n",
    "for i in index:\n",
    "    lat_RCM.append(lat[i])\n",
    "    lon_RCM.append(lon[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the RCM lat and lon\n",
    "# np.savetxt(\"RCM_lat.csv\", lat_RCM, delimiter=\",\")\n",
    "# np.savetxt(\"RCM_lon.csv\", lon_RCM, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data before trying to process it\n",
    "P = P_world.psl.values\n",
    "U = U_world.uas.values\n",
    "V = V_world.vas.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time -pressure done (s):4.190555572509766\n",
      "time -U wind done (s):4.60263729095459\n",
      "time -V wind done (s):4.651688814163208\n"
     ]
    }
   ],
   "source": [
    "#now convert data to 2D\n",
    "start = time.time()\n",
    "P2d = np.empty((len(P_world.rlat)*len(P_world.rlon),len(P_world.time)))\n",
    "               \n",
    "for i in range(len(P_world.time)):\n",
    "    P2d[:,i] = np.reshape(P[i],(len(P_world.rlat)*len(P_world.rlon),))\n",
    "print(f\"time -pressure done (s):\"+str(time.time()-start))\n",
    "\n",
    "               \n",
    "start = time.time()\n",
    "U2d = np.empty((len(U_world.rlat)*len(U_world.rlon),len(U_world.time)))\n",
    "               \n",
    "for i in range(len(U_world.time)):\n",
    "    U2d[:,i] = np.reshape(U[i],(len(U_world.rlat)*len(U_world.rlon),))\n",
    "print(f\"time -U wind done (s):\"+str(time.time()-start))\n",
    "\n",
    "               \n",
    "start = time.time()\n",
    "V2d = np.empty((len(V_world.rlat)*len(V_world.rlon),len(V_world.time)))\n",
    "               \n",
    "for i in range(len(V_world.time)):\n",
    "    V2d[:,i] = np.reshape(V[i],(len(V_world.rlat)*len(V_world.rlon),))\n",
    "print(f\"time -V wind done (s):\"+str(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use indices found in lat-lon step to trim CanRCM4 extent\n",
    "\n",
    "P_RCM = np.empty((len(index),len(P_world.time)))\n",
    "\n",
    "for i in range(len(P_world.time)):\n",
    "    for j in range(len(index)):\n",
    "        P_RCM[j,i] = P2d[index[j],i]\n",
    "        \n",
    "U_RCM = np.empty((len(index),len(U_world.time)))\n",
    "\n",
    "for i in range(len(U_world.time)):\n",
    "    for j in range(len(index)):\n",
    "        U_RCM[j,i] = U2d[index[j],i]\n",
    "        \n",
    "V_RCM = np.empty((len(index),len(V_world.time)))\n",
    "\n",
    "for i in range(len(V_world.time)):\n",
    "    for j in range(len(index)):\n",
    "        V_RCM[j,i] = V2d[index[j],i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove seasonal cycle\n",
    "nyears = 7\n",
    "\n",
    "#pressure\n",
    "seasonal = np.empty((np.shape(P_RCM)[0],365))\n",
    "for ii in range(len(P_RCM)):\n",
    "    seasonal[ii,:] = np.mean(np.reshape(P_RCM[ii,:],(nyears,365)),axis=0)\n",
    "\n",
    "#repeat the seasonal cycle for all years\n",
    "seasonal_all = np.tile(seasonal,(1,nyears))\n",
    "\n",
    "#remove seasonal cycle from the original data (calculate anomalies)\n",
    "P_RCM_anom = P_RCM - seasonal_all\n",
    "\n",
    "#U-wind\n",
    "seasonal = np.empty((np.shape(U_RCM)[0],365))\n",
    "for ii in range(len(U_RCM)):\n",
    "    seasonal[ii,:] = np.mean(np.reshape(U_RCM[ii,:],(nyears,365)),axis=0)\n",
    "\n",
    "#repeat the seasonal cycle for all years\n",
    "seasonal_all = np.tile(seasonal,(1,nyears))\n",
    "\n",
    "#remove seasonal cycle from the original data (calculate anomalies)\n",
    "U_RCM_anom = U_RCM - seasonal_all\n",
    "\n",
    "#V-wind\n",
    "seasonal = np.empty((np.shape(V_RCM)[0],365))\n",
    "for ii in range(len(V_RCM)):\n",
    "    seasonal[ii,:] = np.mean(np.reshape(V_RCM[ii,:],(nyears,365)),axis=0)\n",
    "\n",
    "#repeat the seasonal cycle for all years\n",
    "seasonal_all = np.tile(seasonal,(1,nyears))\n",
    "\n",
    "#remove seasonal cycle from the original data (calculate anomalies)\n",
    "V_RCM_anom = V_RCM - seasonal_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "NOTE : before export find out how to limit range of the PCs so that you don't need to normalize afterwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA on the world data\n",
    "#looking for dominant spatial patterns to be eigenvectors and how those spatial patterns evelove over the month to the the PCs)\n",
    "#need to take the transpose of the matrix \n",
    "\n",
    "data = P_RCM_anom.T\n",
    "\n",
    "n_modes = np.min(np.shape(data))\n",
    "pca = PCA(n_components = n_modes)\n",
    "P_RCM_PCs = pca.fit_transform(data)\n",
    "P_RCM_eigvecs = pca.components_\n",
    "P_RCM_fracVar = pca.explained_variance_ratio_\n",
    "\n",
    "data = U_RCM_anom.T\n",
    "\n",
    "n_modes = np.min(np.shape(data))\n",
    "pca = PCA(n_components = n_modes)\n",
    "U_RCM_PCs = pca.fit_transform(data)\n",
    "U_RCM_eigvecs = pca.components_\n",
    "U_RCM_fracVar = pca.explained_variance_ratio_\n",
    "\n",
    "data = V_RCM_anom.T\n",
    "\n",
    "n_modes = np.min(np.shape(data))\n",
    "pca = PCA(n_components = n_modes)\n",
    "V_RCM_PCs = pca.fit_transform(data)\n",
    "V_RCM_eigvecs = pca.components_\n",
    "V_RCM_fracVar = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the relevant arrays to CSV\n",
    "np.savetxt(\"P_RCM_PCs.csv\", P_RCM_PCs, delimiter=\",\")\n",
    "np.savetxt(\"P_RCM_eigvecs.csv\", P_RCM_eigvecs, delimiter=\",\")\n",
    "np.savetxt(\"P_RCM_fracVar.csv\", P_RCM_fracVar, delimiter=\",\")\n",
    "\n",
    "np.savetxt(\"U_RCM_PCs.csv\", U_RCM_PCs, delimiter=\",\")\n",
    "np.savetxt(\"U_RCM_eigvecs.csv\", U_RCM_eigvecs, delimiter=\",\")\n",
    "np.savetxt(\"U_RCM_fracVar.csv\", U_RCM_fracVar, delimiter=\",\")\n",
    "\n",
    "np.savetxt(\"V_RCM_PCs.csv\", V_RCM_PCs, delimiter=\",\")\n",
    "np.savetxt(\"V_RCM_eigvecs.csv\", V_RCM_eigvecs, delimiter=\",\")\n",
    "np.savetxt(\"V_RCM_fracVar.csv\", V_RCM_fracVar, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA on the HRDPS data \n",
    "data = P_hrdps_anom.T\n",
    "\n",
    "n_modes = np.min(np.shape(data))\n",
    "pca = PCA(n_components = n_modes)\n",
    "P_hrdps_PCs = pca.fit_transform(data)\n",
    "P_hrdps_eigvecs = pca.components_\n",
    "P_hrdps_fracVar = pca.explained_variance_ratio_\n",
    "\n",
    "data = U_hrdps_anom.T\n",
    "\n",
    "n_modes = np.min(np.shape(data))\n",
    "pca = PCA(n_components = n_modes)\n",
    "U_hrdps_PCs = pca.fit_transform(data)\n",
    "U_hrdps_eigvecs = pca.components_\n",
    "U_hrdps_fracVar = pca.explained_variance_ratio_\n",
    "\n",
    "data = P_hrdps_anom.T\n",
    "\n",
    "n_modes = np.min(np.shape(data))\n",
    "pca = PCA(n_components = n_modes)\n",
    "V_hrdps_PCs = pca.fit_transform(data)\n",
    "V_hrdps_eigvecs = pca.components_\n",
    "V_hrdps_fracVar = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the relevant arrays to CSV\n",
    "np.savetxt(\"P_hrdps_PCs.csv\", P_hrdps_PCs, delimiter=\",\")\n",
    "np.savetxt(\"P_hrdps_eigvecs.csv\", P_hrdps_eigvecs, delimiter=\",\")\n",
    "np.savetxt(\"P_hrdps_fracVar.csv\", P_hrdps_fracVar, delimiter=\",\")\n",
    "\n",
    "np.savetxt(\"U_hrdps_PCs.csv\", U_hrdps_PCs, delimiter=\",\")\n",
    "np.savetxt(\"U_hrdps_eigvecs.csv\", U_hrdps_eigvecs, delimiter=\",\")\n",
    "np.savetxt(\"U_hrdps_fracVar.csv\", U_hrdps_fracVar, delimiter=\",\")\n",
    "\n",
    "np.savetxt(\"V_hrdps_PCs.csv\", V_hrdps_PCs, delimiter=\",\")\n",
    "np.savetxt(\"V_hrdps_eigvecs.csv\", V_hrdps_eigvecs, delimiter=\",\")\n",
    "np.savetxt(\"V_hrdps_fracVar.csv\", V_hrdps_fracVar, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save date arrays that you want to work with\n",
    "dates = P_world.time.values\n",
    "np.savetxt(\"RCM_dates.csv\",dates,delimiter=\",\",fmt=\"%s\")\n",
    "dates = hrdps.time_counter.values\n",
    "np.savetxt(\"hrdps_dates.csv\",dates,delimiter=\",\",fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(P_hrdps_PCs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eosc510",
   "language": "python",
   "name": "eosc510"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
