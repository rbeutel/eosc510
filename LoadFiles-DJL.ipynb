{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Save Files for CanRCM4 Downscaling Project\n",
    "Load in and process HRDPS files here then export them as netcdf or numpy arrays\n",
    "\n",
    "*This version is the 1st part of code review changes and comments by Doug of `LoadFiles.ipynb`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DJL Comments on Cell Above\n",
    "\n",
    "Be careful about aliasing imports to names that you might want to use as variable names like `dt`.\n",
    "If, somewhere down in the notebook, you forget that you did `import datetime as dt` here,\n",
    "and do something like `dt = t2 - t1` you will have re-assigned the value of `dt` and it could be \n",
    "very hard to figure out why your later use of `dt` to mean the `datatime` module isn't working.\n",
    "\n",
    "The `pd`, `np`, and `xr` aliases are all pretty common, and not immediately obvious variables names\n",
    "(though context matters!) so they are probably okay.\n",
    "Personally, I rarely use import aliases because tab completion in Jupyter and my IDE mean that I don't have to\n",
    "type more than a few letters, so why bother aliasing.\n",
    "\n",
    "You are not using `pandas` anywhere in this notebook, so don't import it.\n",
    "I realize that it is a carry over from you factoring this part of your processing pipeline out of\n",
    "`proj-slp.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DJL Comments\n",
    "\n",
    "I'm going to split the next cell into 2 cells to comment on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HRDPS data for all of 2016 to end of 2019\n",
    "path = Path(\"/home/rbeutel/analysis/eosc510/proj.ipynb\").resolve().parents[2]\n",
    "path2 = \"/results/forcing/atmospheric/GEM2.5/operational/\"\n",
    "path = path/path2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DJL Comments on Cell Above\n",
    "\n",
    "I'm not sure what you are trying to do with `path` and `path2`.\n",
    "The way `pathlib.Path` works means that, because `path2` starts with a `/`,\n",
    "the `path = path/path2` statement will only assign the `path2` part to `path`.\n",
    "\n",
    "I would write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrdps_path = Path(\"/results/forcing/atmospheric/GEM2.5/operational/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DJL Comments on Cell Below\n",
    "(because it has so many PerformanceWarnings)\n",
    "\n",
    "Your glob expression is returning *a lot* more than the 2016-2019 files!\n",
    "It is getting 2010-2019. The `*` means \"any characters\".\n",
    "That probably explains why it was consuming so much memory on `salish`.\n",
    "To get 2016-2019 you need to use `\"ops_y201[6-9]*.nc\"`. \n",
    "I was hoping that I could point you to a section in ch 7 of \"The Linux Commandline\" that \n",
    "would explain the way I'm using square brackets there, but it is not covered in that chapter.\n",
    "Instead, you have to dive into ch 19 \"Regular Expressions\" and look at the \n",
    "\"Bracket Expressions and Character Classes\" section on pg 220.\n",
    "\n",
    "For faster testing I would use glob expressions that select a few days, a month, or a few months; examples:\n",
    "* `\"ops_y2016m01d1[0-9]*.nc` will return 10 days in Jan 2016\n",
    "* `\"ops_y2016m03*.nc` will return Mar 2016\n",
    "* `\"ops_y2016m0[456]*.nc` or `\"ops_y2016m0[4-6]*.nc` will return Apr-Jun 2016\n",
    "\n",
    "You can test your glob expressions using `ls` in the shell.\n",
    "You can even run a shell command from a notebook by putting a `!` in front of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/results/forcing/atmospheric/GEM2.5/operational/ops_y2016m01d10.nc\n",
      "/results/forcing/atmospheric/GEM2.5/operational/ops_y2016m01d11.nc\n",
      "/results/forcing/atmospheric/GEM2.5/operational/ops_y2016m01d12.nc\n",
      "/results/forcing/atmospheric/GEM2.5/operational/ops_y2016m01d13.nc\n",
      "/results/forcing/atmospheric/GEM2.5/operational/ops_y2016m01d14.nc\n",
      "/results/forcing/atmospheric/GEM2.5/operational/ops_y2016m01d15.nc\n",
      "/results/forcing/atmospheric/GEM2.5/operational/ops_y2016m01d16.nc\n",
      "/results/forcing/atmospheric/GEM2.5/operational/ops_y2016m01d17.nc\n",
      "/results/forcing/atmospheric/GEM2.5/operational/ops_y2016m01d18.nc\n",
      "/results/forcing/atmospheric/GEM2.5/operational/ops_y2016m01d19.nc\n"
     ]
    }
   ],
   "source": [
    "!ls /results/forcing/atmospheric/GEM2.5/operational/ops_y2016m01d1[0-9]*.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrdps = xr.open_mfdataset(sorted(hrdps_path.glob(\"ops_y201[6-9]*.nc\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DJL Comments\n",
    "\n",
    "The cell above is still slow (many minutes) and consumes a lot of memory (>100 Gb ???).\n",
    "It is also the point where this process starts using [dask](https://docs.dask.org/) to\n",
    "process files in parallel and manage the fact that 4 years of HRDPS are too large to fit\n",
    "in memory.\n",
    "`dask` handles that by splitting the processing up into a collection\n",
    "(formally a graph) of tasks, and deferring execution of those tasks until the latest\n",
    "possible time. In this case that is when you do the `np.concatenate()` operations below.\n",
    "\n",
    "The challenge with `dask` (and parallel processing in general) is that it has to be tuned\n",
    "for each particular workflow. All of my reading and experience of trying to use `dask` and\n",
    "other parallel processing things has lead me to believe that there are few general rules for\n",
    "how to do that tuning, mostly just guidelines.\n",
    "\n",
    "So, I spent a bunch of interesting time yesterday afternoon trying to figure out how best to \n",
    "tune things for this next bit of processing. I'm not there yet, but I am making progress.\n",
    "I decided to break this review here so that you could have the benefit of what is above\n",
    "while I sort out the `dask` story.\n",
    "\n",
    "To be continued..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next we want the HRDPS data to be daily instead of hourly\n",
    "day_avg = hrdps.resample(time_counter='D').mean(dim='time_counter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find index of extra day (leap day)\n",
    "ind = list(day_avg.time_counter.values).index(np.datetime64('2016-02-29T00:00:00.000000000'))\n",
    "\n",
    "#and then trim to be in a more relevant extent and remove leap day\n",
    "#(dont need the HRDPS data to stretch as far as it does inland)\n",
    "hrdps_ssc = day_avg.sel(x=slice(0., 480000.),time_counter=slice('2016-01-01 12:00:00', '2019-12-31 12:00:00'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrdps_P = np.concatenate((hrdps_ssc.atmpres.values[0:ind,:,:], hrdps_ssc.atmpres.values[(ind+1):,:,:]))\n",
    "hrdps_U = np.concatenate((hrdps_ssc.u_wind.values[0:ind,:,:], hrdps_ssc.u_wind.values[(ind+1):,:,:]))\n",
    "hrdps_V = np.concatenate((hrdps_ssc.v_wind.values[0:ind,:,:], hrdps_ssc.v_wind.values[(ind+1):,:,:]))\n",
    "hrdps_T = np.concatenate((hrdps_ssc.tair.values[0:ind,:,:], hrdps_ssc.tair.values[(ind+1):,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "different number of dimensions on data and dims: 3 vs 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-203b9d8a9732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_c\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     da[var] = xr.DataArray(\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_c\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/eosc510/lib/python3.9/site-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, coords, dims, name, attrs, indexes, fastpath)\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_data_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_compatible_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_coords_and_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             indexes = dict(\n",
      "\u001b[0;32m~/anaconda3/envs/eosc510/lib/python3.9/site-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36m_infer_coords_and_dims\u001b[0;34m(shape, coords, dims)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0;34m\"different number of dimensions on data \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;34m\"and dims: %s vs %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: different number of dimensions on data and dims: 3 vs 2"
     ]
    }
   ],
   "source": [
    "# set up titles\n",
    "netcdf_title = 'HRDPSsubset.nc'\n",
    "netcdf_comment = 'HRDPS2.5 datast used in CanRCM4 downscaling attempt for the Salish Sea'\n",
    "notebook = 'LoadFiles.ipynb'\n",
    "\n",
    "ds_attrs = {\n",
    "        'creator_email':\n",
    "            'rbeutel@eoas.ubc.ca',\n",
    "        'institution_fullname': (\n",
    "            'Earth, Ocean & Atmospheric Sciences,'\n",
    "            ' University of British Columbia'\n",
    "        ),\n",
    "    'title': netcdf_title,\n",
    "    'comment': netcdf_comment,\n",
    "    'notebook': notebook,\n",
    "    'summary': f'sea-level pressure, N/S wind, E/W wind, temperature',\n",
    "    'history': (\n",
    "            '[{}] File creation.'\n",
    "            .format(dt.datetime.today().strftime('%Y-%m-%d'))\n",
    "        )\n",
    "}\n",
    "\n",
    "coords_c = {\n",
    "    'x' : np.arange(hrdps.x.shape[0]),\n",
    "    'y' : np.arange(hrdps.y.shape[0]),\n",
    "}\n",
    "\n",
    "coords = {\n",
    "    'x' : np.arange(hrdps.x.shape[0]),\n",
    "    'y' : np.arange(hrdps.y.shape[0]),\n",
    "    'time_counter' : hrdps.time_counter.values\n",
    "}\n",
    "\n",
    "data_c = {}\n",
    "var_attrs_c = {} \n",
    "\n",
    "data_c['nav_lat'] = hrdps.nav_lat.values\n",
    "var_attrs_c['nav_lat'] = {'units': 'degrees_north',\n",
    "                       'long_name': 'latitude'}\n",
    "\n",
    "data_c['nav_lon'] = hrdps.nav_lon.values\n",
    "var_attrs_c['nav_lon'] = {'units': 'degrees_east',\n",
    "                       'long_name': 'longitude'}\n",
    "\n",
    "da = {}\n",
    "for var in data_c:\n",
    "    da[var] = xr.DataArray(\n",
    "        data = data_c[var],\n",
    "        name=var,\n",
    "        dims=('y', 'x'),\n",
    "        coords = coords_c,\n",
    "        attrs = var_attrs_c[var])\n",
    "\n",
    "data = {}\n",
    "var_attrs = {}\n",
    "\n",
    "var_attrs['slp'] = {'units': 'Pa',\n",
    "                      'long_name': 'Pressure Reduced to MLS [Pa]'}\n",
    "data['slp'] = hrdps_P\n",
    "\n",
    "var_attrs['u_wind'] = {'units': 'm/s',\n",
    "                      'long_name': 'U component of wind [m/s]'}\n",
    "data['u_wind'] = hrdps_U\n",
    "\n",
    "var_attrs['v_wind'] = {'units': 'm/s',\n",
    "                      'long_name': 'V component of wind [m/s]'}\n",
    "data['v_wind'] = hrdps_V\n",
    "\n",
    "var_attrs['temp'] = {'units': 'oC',\n",
    "                      'long_name': 'Surface temperature [oC]'}\n",
    "data['temp'] = hrdps_T\n",
    "\n",
    "for var in data:\n",
    "    da[var] = xr.DataArray(\n",
    "        data = data[var],\n",
    "        name=var,\n",
    "        dims=('time_counter', 'y', 'x'),\n",
    "        coords = coords,\n",
    "        attrs = var_attrs[var])\n",
    "    \n",
    "\n",
    "ds = xr.Dataset(\n",
    "        data_vars={\n",
    "            'nav_lat': da['nav_lat'],\n",
    "            'nav_lon': da['nav_lon'],\n",
    "            'slp': da['slp'],\n",
    "            'u_wind': da['u_wind'],\n",
    "            'v_wind': da['v_wind'],\n",
    "            'temp': da['temp']},\n",
    "        coords = coords,\n",
    "        attrs = ds_attrs\n",
    ")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
